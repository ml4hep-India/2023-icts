{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZT7AoY8sKwp1"
   },
   "source": [
    "# Hands on : Introduction to BDT on HEP dataset\n",
    "\n",
    "1. Load data from root (make sure packages load!)\n",
    "2. Explore the data and weights\n",
    "3. Preprocess data for training\n",
    "4. Train a Boosted Decision Tree\n",
    "5. Quantify its performance\n",
    "\n",
    "This tutorial will probably be a bit more hands-on, focusing a bit more on the data and preprocessing. The Neural Network tutorial later today will give more time to play around with training. Feel free to revisit this notebook later today! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many thanks to _Fernando Acosta_, _David Rousseau, Yann Coadou_ for their contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT7-MMpfrlHR"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xy-_72FJKwp6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uproot as ur\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns', 100)  # to see more columns of df.head()\n",
    "np.random.seed(31415)  # set the np random seed for the reproducibility\n",
    "\n",
    "# some utilities\n",
    "from math import sqrt\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oF367-yfKwp9",
    "outputId": "9d1c1e61-83c7-4ec5-dd41-bc099c72cdb2"
   },
   "outputs": [],
   "source": [
    "# import xgboost\n",
    "\n",
    "#print(xgboost.__version__)  # Tested with 1.6.1, version above 1 is recommended.\n",
    "import lightgbm\n",
    "\n",
    "print(lightgbm.__version__)  # Tested with 2.2.3\n",
    "import sklearn\n",
    "\n",
    "print(sklearn.__version__)  # Tested with 1.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g14yIjbPKwqC"
   },
   "source": [
    "# Load events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[created from [ATLAS Open Data](http://opendata.atlas.cern/release/2020/documentation/datasets/intro.html)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"dataWW_d1.root\"\n",
    "file = ur.open(filename)\n",
    "print(file.classnames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = file[\"tree_event\"]\n",
    "dfall = tree.arrays(library=\"pd\")\n",
    "print(\"File loaded with \", dfall.shape[0], \" events \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kl-4W4Ifs8EV",
    "outputId": "fdc0b348-19f9-4796-dd62-b95321e84b2a"
   },
   "outputs": [],
   "source": [
    "# shuffle the events, already done but just to be safe!\n",
    "dfall = dfall.sample(frac=1).reset_index(drop=True)\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"now :\", datetime.now())\n",
    "print(\"File loaded with \", dfall.shape[0], \" events \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uf9Gt8g8KwqF"
   },
   "source": [
    "At this point, it should tell you \"File Loaded with XXX events\". If not, it could not access the datafile. No point going further!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67cCYsYIKwqG"
   },
   "source": [
    "# Examine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sx1_PB22KwqG",
    "outputId": "2113a2f2-28c2-437d-e17b-e52563e222a5"
   },
   "outputs": [],
   "source": [
    "# dump list of features\n",
    "dfall.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kl5MAys-yd0R"
   },
   "outputs": [],
   "source": [
    "dfall.mcWeight *= 4  # arbitrary scale to have larger significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "j9l7wkkorlHe",
    "outputId": "cd9c6e9a-aa2f-4ca9-e51b-a121fe7bc990"
   },
   "outputs": [],
   "source": [
    "# examine first few events\n",
    "display(dfall.head())\n",
    "display(dfall.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "Oz-lWJhgrlHg",
    "outputId": "cd638631-5a63-4ee3-a3d8-e92b1b18b809"
   },
   "outputs": [],
   "source": [
    "# examine feature distribution\n",
    "dfall.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQsalTmorlHj",
    "outputId": "99c7d019-8d11-41f7-da25-a1f7397a8723"
   },
   "outputs": [],
   "source": [
    "label_weights = (dfall[dfall.label == 0].mcWeight.sum(), dfall[dfall.label == 1].mcWeight.sum())\n",
    "print(\"sum of label weights  Background, Signal =\", label_weights)\n",
    "\n",
    "label_nevents = (dfall[dfall.label == 0].shape[0], dfall[dfall.label == 1].shape[0])\n",
    "print(\"total class number of events B S\", label_nevents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtI5u5GErlHq"
   },
   "source": [
    "## Event selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook essentially tries to classify events containing a Higgs Boson.\n",
    "\n",
    "The simulation includes top-quark-pair production, single-top production, production of weak bosons in association with jets ($W$+jets, $Z$+jets), production of a pair of bosons (diboson $WW, WZ, ZZ$) and __SM Higgs ($\\to WW$)__ production.\n",
    "\n",
    "We will only keep events with exactly two leptons __dfall.lep_n==2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaO2JM1hrlHr",
    "outputId": "8ebaa97b-a097-4cb0-a1ad-6e4d9847b850"
   },
   "outputs": [],
   "source": [
    "print(\"Df shape before selection :\", dfall.shape)\n",
    "\n",
    "# Sometimes we keep only events with positive weight. This is in principle wrong.\n",
    "# Many Data Science tools break given a negative weight, like XGBoost, but LightGBM can handle it.\n",
    "# fulldata = dfall[(dfall.lep_n == 2) & (dfall.mcWeight > 0)]\n",
    "fulldata = dfall[(dfall.lep_n == 2)]\n",
    "\n",
    "\n",
    "print(\"Df shape after selection :\", fulldata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nviyIMgerlH3"
   },
   "source": [
    "\n",
    "### Try not to change the cells above $\\uparrow$\n",
    "...and return to this cell (or rerun the whole notebook) after changing things below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose features to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "6e0Hlpv6rlH4",
    "outputId": "387bb054-8c0c-47e2-efc0-0e4f3fefcd38",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WARNING : there should be no selection nor shuffling later on ! (otherwise misalignement)\n",
    "target = fulldata[\"label\"]\n",
    "weights = fulldata[\"mcWeight\"]\n",
    "\n",
    "\n",
    "# for simplicity of the exercise only keep some features\n",
    "data = pd.DataFrame(fulldata, columns=[\"met_et\", \"met_phi\", \"lep_pt_0\", \"lep_pt_1\", 'lep_phi_0', 'lep_phi_1'])\n",
    "\n",
    "print(\"Df shape of dataset to be used :\", data.shape)\n",
    "display(data.head())\n",
    "display(target.head())\n",
    "display(weights.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "IJAmtop6yd0a",
    "outputId": "6a1e149d-cb59-48d9-d19f-25a8efe6735f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Simple Histo of ET for pT sum > 1000\n",
    "data[data.lep_pt_0 + data.lep_pt_1 > 1000]['met_et'].plot.hist(\n",
    "    bins=np.linspace(0, 400, 100), ax=axes[0], title='Missing Transverse Energy for large lepton Pt'\n",
    ")\n",
    "\n",
    "\n",
    "# Scatter of pT vs. ET\n",
    "fig = plt.figure()\n",
    "data[target == 0].plot.scatter(x='met_et', y='lep_pt_0', color=\"b\", label=\"B\", ax=axes[1])\n",
    "data[target == 1].plot.scatter(x='met_et', y='lep_pt_0', color=\"r\", label=\"S\", ax=axes[1])\n",
    "axes[1].set_title(\"Lepton $p_\\mathrm{T}\\ vs.\\ Missing E_\\mathrm{T}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "px76_qOlyd0c",
    "outputId": "b00949c1-67a6-46d0-d459-296c640d6079",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simple example of pandas array \"slicing\"\n",
    "data[data.lep_pt_0 + data.lep_pt_1 > 2000].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVHrJtodL2wo"
   },
   "source": [
    "## Examine the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "cFD4s616LS0F",
    "outputId": "f9c3a54e-8950-4b15-c2c8-fb4b11d346b4"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# fig=plt.figure()\n",
    "\n",
    "bins = np.linspace(-1, 3, 101)\n",
    "plt.hist(weights[target == 0] * 1000, bins=bins, color='b', alpha=0.5, density=True, label='[B]ackground')\n",
    "plt.hist(weights[target == 1] * 1000, bins=bins, color='r', alpha=0.5, density=True, label='[S]ignal')\n",
    "plt.legend(loc='best')\n",
    "ax.set_xlabel('weight*1000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrCZdWvqyd0e"
   },
   "source": [
    "# Some weight studies \n",
    "\n",
    "$s=\\sum w$ for signal dataset : predicted number of signal events (luminosity, cross section, efficiencies etc... already includded in the weights). Ditto for background, $b$. \n",
    "\n",
    "\n",
    "Effective number of events fraction : $\\frac{N_{eff}}{N}= \\frac{1}{1+\\frac{Var(w)}{<w>^2}}$ . Example : if 0.2 it means the precision achieved with this dataset is the one which would be achieved with an unweighted dataset of 0.2 x N events (this is a rough estimate, only true for a simple counting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdpzI4-6MVIK",
    "outputId": "bed22cff-ee33-463d-ea3d-7c91d5a8c96a"
   },
   "outputs": [],
   "source": [
    "label_n_weights = np.zeros(2)\n",
    "label_sum_weights = np.zeros(2)\n",
    "label_mean_weights = np.zeros(2)\n",
    "label_std_weights = np.zeros(2)\n",
    "label_neff_fraction = np.zeros(2)\n",
    "label_sum_weightsSqr = np.zeros(2)\n",
    "\n",
    "for i in range(2):\n",
    "    label_n_weights[i] = weights[target == i].size\n",
    "    label_mean_weights[i] = weights[target == i].mean()\n",
    "    label_std_weights[i] = weights[target == i].std()\n",
    "    label_sum_weights[i] = weights[target == i].sum()\n",
    "    label_sum_weightsSqr[i] = (weights[target == i] **2).sum()\n",
    "Neffective = label_sum_weights**2 / label_sum_weightsSqr\n",
    "\n",
    "print(\"Weights quantities for background (target==0) and signal (target==1)\")\n",
    "print(\"Weights sum\", np.round(label_sum_weights, 1))\n",
    "print(\"N events\", label_n_weights)\n",
    "#print(\"Weights mean\", label_mean_weights)\n",
    "#print(\"Weights std\", label_std_weights)\n",
    "print(\"Effective number of events\", np.round(Neffective, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F77Cx9LhKwqc",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Feature engineering (Two variations)\n",
    "To be switched on in a second iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. See if using more features improves model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_features = False\n",
    "if more_features:\n",
    "    data = pd.DataFrame(\n",
    "        fulldata,\n",
    "        columns=[\n",
    "            \"met_et\",\n",
    "            \"met_phi\",\n",
    "            \"lep_pt_0\",\n",
    "            \"lep_pt_1\",\n",
    "            'lep_eta_0',\n",
    "            'lep_eta_1',\n",
    "            'lep_phi_0',\n",
    "            'lep_phi_1',\n",
    "            'jet_n',\n",
    "            'jet_pt_0',\n",
    "            'jet_pt_1',\n",
    "            'jet_eta_0',\n",
    "            'jet_eta_1',\n",
    "            'jet_phi_0',\n",
    "            'jet_phi_1',\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Engineer our own feature, $\\Delta\\varphi_l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oe_9ZEEKwqc"
   },
   "outputs": [],
   "source": [
    "do_feature_engineering = False\n",
    "if do_feature_engineering:\n",
    "    data[\"lep_deltaphi\"] = np.abs(np.mod(data.lep_phi_1 - data.lep_phi_0 + 3 * np.pi, 2 * np.pi) - np.pi)\n",
    "    # data[\"lep_deltaphi\"]=data.lep_phi_1-data.lep_phi_0\n",
    "\n",
    "    print(data.shape)\n",
    "    display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDp3D2Fkyd0h"
   },
   "source": [
    "# Plot the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "id": "o2mf1bLVrlH7",
    "outputId": "0235ba71-8c23-4819-ca7e-5cd4ca301478"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "ax = data[target == 0].hist(weights=weights[target == 0], figsize=(15, 12), color='b', alpha=0.5, density=True, label=\"B\")\n",
    "ax = ax.flatten()[: data.shape[1]]  # to avoid error if holes in the grid of plots (like if 7 or 8 features)\n",
    "ax = data[target == 1].hist(\n",
    "    weights=weights[target == 1], figsize=(15, 12), color='r', alpha=0.5, density=True, ax=ax, label=\"S\"\n",
    ")\n",
    "# ax.legend(loc=\"best\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUMC85s4yd0i"
   },
   "source": [
    "### Features correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 631
    },
    "id": "Kb-U3Ri5yd0i",
    "outputId": "9a8bee46-42a4-4b12-f05a-8fb9237523c5"
   },
   "outputs": [],
   "source": [
    "import seaborn as sn  # seaborn for plots with more appealing defaults\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(15, 5))\n",
    "corrMatrix = data[target == 1].corr()\n",
    "sn.heatmap(corrMatrix, annot=True, ax=axes[0])\n",
    "axes[0].set_title(\"Signal Feature Correlation Matrix\", fontsize=18)\n",
    "\n",
    "# print (\"Background feature correlation matrix\")\n",
    "corrMatrix = data[target == 0].corr()\n",
    "sn.heatmap(corrMatrix, annot=True, ax=axes[1])\n",
    "_ = axes[1].set_title(\"Background Feature Correlation Matrix\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxybCOi-rlIM"
   },
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kowHjX4rlIC"
   },
   "source": [
    "## Split Data into Test and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(31415)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_size = 0.75  # fraction of sample used for training\n",
    "\n",
    "X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(\n",
    "    data, target, weights, train_size=train_size\n",
    ")\n",
    "\n",
    "# Reset index for dataseries, not needed for ndarray (X_train, X_test)\n",
    "# Basically just re-adding the original element indexing from pandas\n",
    "y_train, y_test, weights_train, weights_test = (\n",
    "    y_train.reset_index(drop=True),\n",
    "    y_test.reset_index(drop=True),\n",
    "    weights_train.reset_index(drop=True),\n",
    "    weights_test.reset_index(drop=True),\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"weights shape:\", weights_train.shape, \"\\n\")\n",
    "\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"weight shape:\", weights_test.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Standardize the Data\n",
    "\n",
    "**Scale to Mean of 0 and Variance of 1.0:**   $\\ \\ \\ \\ (x-\\mu)/\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not usually needed for BDT but is good practice\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)  # applies the transformation calculated the line above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust the Test and Train Signal/Background Weights\n",
    "Train on equal amount of Signal and Background, Test on 'natural' ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9j5hdrmrlID",
    "outputId": "195b5c20-3783-4e55-b124-08839bbcfcab"
   },
   "outputs": [],
   "source": [
    "class_weights_train = (weights_train[y_train == 0].sum(), weights_train[y_train == 1].sum())\n",
    "\n",
    "for i in range(len(class_weights_train)):  # loop on B then S targets(labels)\n",
    "    # training dataset: equalize number of background and signal\n",
    "    weights_train[y_train == i] *= max(class_weights_train) / class_weights_train[i]\n",
    "\n",
    "    # test dataset : increase test weight to compensate for sampling\n",
    "    weights_test[y_test == i] *= 1 / (1 - train_size)\n",
    "\n",
    "print(\"Weights have been normalised to a given number of proton collision\")\n",
    "print(\"Orig : total weight sig\", weights[target == 1].sum())\n",
    "print(\"Orig : total weight bkg\", weights[target == 0].sum(), \"\\n\")\n",
    "\n",
    "print(\"Test : total weight sig\", weights_test[y_test == 1].sum())\n",
    "print(\"Test : total weight bkg\", weights_test[y_test == 0].sum(), \"\\n\")\n",
    "print(\"Train : total weight sig\", weights_train[y_train == 1].sum())\n",
    "print(\"Train : total weight bkg\", weights_train[y_train == 0].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with LightGBM (handles negative weighted events unlike XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(31415)  # set the random seed\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score  # for binary classification if x > 0.5 -> 1 else -> 0\n",
    "\n",
    "# gbm = lgb.LGBMClassifier()\n",
    "gbm = lgb.LGBMClassifier() # HPO, check on the web https://lightgbm.readthedocs.io/ for other parameters\n",
    "\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "gbm.fit(X_train, y_train.values, sample_weight=weights_train.values)\n",
    "\n",
    "\n",
    "training_time = time.time() - starting_time\n",
    "print(\"Training time:\", training_time)\n",
    "\n",
    "y_pred_gbm = gbm.predict_proba(X_test)[:, 1]\n",
    "y_pred_gbm = y_pred_gbm.ravel()\n",
    "y_pred_train_gbm = gbm.predict_proba(X_train)[:, 1].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easier to remove negative weighted events for fast AUC calucaltion\n",
    "posWeightsTrain, posWeightsTest = weights_train>0, weights_test>0\n",
    "\n",
    "auc_test_gbm = roc_auc_score(y_true=y_test[posWeightsTest], \n",
    "                             y_score=y_pred_gbm[posWeightsTest], \n",
    "                             sample_weight=weights_test[posWeightsTest])\n",
    "print(\"auc test:\", auc_test_gbm)\n",
    "print(\"auc train:\", roc_auc_score(y_true=y_train.values[posWeightsTrain], \n",
    "                                  y_score=y_pred_train_gbm[posWeightsTrain], \n",
    "                                  sample_weight=weights_train[posWeightsTrain]))\n",
    "print(\"auc test without weights\", roc_auc_score(y_true=y_test, y_score=y_pred_gbm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIsMSGl-Kwql"
   },
   "source": [
    "## Significance Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{med}[Z_0|1] = \\sqrt{q_{0,A}} = \\sqrt{2+((s+b)\\ln(1+s/b)-s)}$\n",
    "\n",
    "**asimov significance [arXiv:1007.1727](https://arxiv.org/pdf/1007.1727.pdf) [Eq. 97]**\n",
    "\n",
    "asimov for significance. Need to esimate your sensitivity to MC. Need thousands of toy tests everytime you use simulation, you need to test your sensitivity. Running a toy MC thousands of times, should converge to 'truth'. Asimov is representative of. Number of sigmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qubY3CMNKwql"
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from math import log\n",
    "\n",
    "\n",
    "def amsasimov(s, b):\n",
    "    if b <= 0 or s <= 0:\n",
    "        return 0\n",
    "    try:\n",
    "        return sqrt(2 * ((s + b) * log(1 + float(s) / b) - s))\n",
    "    except ValueError:\n",
    "        print(1 + float(s) / b)\n",
    "        print(2 * ((s + b) * log(1 + float(s) / b) - s))\n",
    "    # return s/sqrt(s+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum S & B after certain thresholds\n",
    "int_pred_test_sig_gbm = [weights_test[(y_test == 1) & (y_pred_gbm > th_cut)].sum() for th_cut in np.linspace(0, 1, num=50)]\n",
    "int_pred_test_bkg_gbm = [weights_test[(y_test == 0) & (y_pred_gbm > th_cut)].sum() for th_cut in np.linspace(0, 1, num=50)]\n",
    "\n",
    "vamsasimov_gbm = [amsasimov(sumsig, sumbkg) for (sumsig, sumbkg) in zip(int_pred_test_sig_gbm, int_pred_test_bkg_gbm)]\n",
    "significance_gbm = max(vamsasimov_gbm)\n",
    "Z = significance_gbm\n",
    "print(\"Z:\", Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 1, num=50), vamsasimov_gbm, label='LGBM (Z = {})'.format(np.round(significance_gbm, decimals=2)))\n",
    "\n",
    "\n",
    "plt.title(\"BDT Significance\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Significance\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Significance_gbm.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extra_functions import compare_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_train_test(\n",
    "    y_pred_train_gbm,\n",
    "    y_train,\n",
    "    y_pred_gbm,\n",
    "    y_test,\n",
    "    xlabel=\"LGBM score\",\n",
    "    title=\"LGBM\",\n",
    "    weights_train=weights_train.values,\n",
    "    weights_test=weights_test.values,\n",
    ")\n",
    "\n",
    "plt.savefig(\"Score_BDT_GBM_Hist.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrlOjl-Dyd0m",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Hyper Parameter Optimisation\n",
    "**Come back to this if done early**\n",
    "\n",
    "- Can be done by hand or with [random search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a while. Feel free to come back here after finishing\n",
    "do_HP_optimization = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JyTm1fsyd0n",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RandomSearchCV for advanced HPO\n",
    "import scipy.stats as stats\n",
    "\n",
    "if do_HP_optimization:\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    # specify parameters, range, and distributions to sample from\n",
    "    param_dist_XGB = {\n",
    "        'max_depth': stats.randint(3, 12),  # default 6\n",
    "        'n_estimators': stats.randint(300, 800),  # default 100\n",
    "        'learning_rate': stats.uniform(0.1, 0.5),\n",
    "    }  # def 0.3\n",
    "\n",
    "    # default CV is 5 fold, reduce to 2 for speed concern\n",
    "    gsearch = RandomizedSearchCV(\n",
    "        estimator=XGBClassifier(tree_method=\"hist\", use_label_encoder=False, eval_metric='logloss'),\n",
    "        param_distributions=param_dist_XGB,\n",
    "        scoring='roc_auc',\n",
    "        n_iter=10,\n",
    "        cv=2,\n",
    "    )\n",
    "    gsearch.fit(X_train, y_train, sample_weight=weights_train)\n",
    "\n",
    "    print(\"Best parameters : \", gsearch.best_params_)\n",
    "    print(\"Best score (on train dataset CV) : \", gsearch.best_score_)\n",
    "\n",
    "    y_pred_gs = gsearch.predict_proba(X_test)[:, 1]\n",
    "    print(\n",
    "        \"... corresponding score on test dataset : \",\n",
    "        roc_auc_score(y_true=y_test, y_score=y_pred_gs, sample_weight=weights_test),\n",
    "    )\n",
    "    dfsearch = pd.DataFrame.from_dict(gsearch.cv_results_)\n",
    "    display(dfsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7xlfXkPyd0n"
   },
   "outputs": [],
   "source": [
    "if do_HP_optimization:\n",
    "    dfsearch.plot.scatter(\"param_n_estimators\", \"mean_test_score\")\n",
    "    dfsearch.plot.scatter(\"param_max_depth\", \"mean_test_score\")\n",
    "    dfsearch.plot.scatter(\"param_learning_rate\", \"mean_test_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8kJjKBUyd0n"
   },
   "source": [
    "# Learning Curve\n",
    "This could be done with sklearn  [learning_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html).\n",
    "However, older versions cannot handle weights, and therefore do not allow you to control testing dataset size. Need to check newer versions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "Do_Learning_Curve = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dV6OalSjyd0o"
   },
   "outputs": [],
   "source": [
    "if Do_Learning_Curve:\n",
    "    #train_sizes = [0.01, 0.05, 0.1, 0.2, 0.5, 0.75, 1]\n",
    "    #train_sizes = [0.001, 0.003, 0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "    train_sizes = [0.005, 0.02, 0.05, 0.1, 0.2, 0.5, 1.]\n",
    "    ntrains = []\n",
    "    test_aucs = []\n",
    "    train_aucs = []\n",
    "    times = []\n",
    "    fpr = []\n",
    "    tpr = []\n",
    "\n",
    "    for train_size in train_sizes:\n",
    "        ntrain = int(len(X_train) * train_size)\n",
    "        print(\"training with \", ntrain, \" events\")\n",
    "        ntrains += [ntrain]\n",
    "        starting_time = time.time()\n",
    "\n",
    "        # train using the first ntrain event of the training dataset\n",
    "        gbm.fit(X_train[:ntrain,], y_train[:ntrain], sample_weight=weights_train[:ntrain])\n",
    "        training_time = time.time() - starting_time\n",
    "        times += [training_time]\n",
    "\n",
    "        # score on test dataset (always the same)\n",
    "        y_pred_gbm = gbm.predict_proba(X_test)[:, 1]\n",
    "        auc_test_gbm = roc_auc_score(y_true=y_test[posWeightsTest], \n",
    "                                     y_score=y_pred_gbm[posWeightsTest], \n",
    "                                     sample_weight=weights_test[posWeightsTest])\n",
    "        test_aucs += [auc_test_gbm]\n",
    "\n",
    "        # score on the train dataset\n",
    "        y_train_gbm = gbm.predict_proba(X_train[:ntrain])[:, 1]\n",
    "        auc_train_gbm = roc_auc_score(y_true=y_train[:ntrain][weights_train[:ntrain]>0], \n",
    "                                      y_score=y_train_gbm[weights_train[:ntrain]>0], \n",
    "                                      sample_weight=weights_train[:ntrain][weights_train[:ntrain]>0])\n",
    "        train_aucs += [auc_train_gbm]\n",
    "        dflearning = pd.DataFrame({\"Ntraining\": ntrains, \"test_auc\": test_aucs, \"train_auc\": train_aucs, \"time\": times})\n",
    "\n",
    "        #temp_fpr, temp_tpr, _ = roc_curve(y_true=y_train[:ntrain], y_score=y_train_gbm, sample_weight=weights_train[:ntrain])\n",
    "        temp_fpr, temp_tpr, _ = roc_curve(y_true=y_test, y_score=y_pred_gbm, sample_weight=weights_test)\n",
    "        fpr += [temp_fpr]\n",
    "        tpr += [temp_tpr]\n",
    "\n",
    "    display(dflearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWH3s-0Wyd0o"
   },
   "outputs": [],
   "source": [
    "if Do_Learning_Curve:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    lw = 2\n",
    "    # axes[0].plot(fpr_xgb, tpr_xgb, color='blue',lw=lw, label='XGBoost (AUC  = {})'.format(np.round(auc_test_xgb,decimals=2)))\n",
    "    axes[0].plot(fpr[0], tpr[0], color='blue', lw=lw, label='NTrain = %i' % (ntrains[0]))\n",
    "    axes[0].plot(fpr[1], tpr[1], color='red', lw=lw, label='NTrain = %i' % (ntrains[1]))\n",
    "    axes[0].plot(fpr[3], tpr[3], color='green', lw=lw, label='NTrain = %i' % (ntrains[3]))\n",
    "    axes[0].plot(fpr[4], tpr[4], color='orange', lw=lw, label='NTrain = %i' % (ntrains[4]))\n",
    "    axes[0].plot(fpr[6], tpr[6], color='black', lw=lw, label='NTrain = %i' % (ntrains[6]))\n",
    "\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title('Receiver Operating Characteristic')\n",
    "\n",
    "    axes[1].set_title(\"Test AUC vs. NTraining\")\n",
    "    dflearning.plot.scatter(\"Ntraining\", \"test_auc\", ax=axes[1])\n",
    "    # focus on the last point\n",
    "    # dflearning[4:].plot.scatter(\"Ntraining\",\"test_auc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance\n",
    "Feature importance allows to display the importance of each feature without rerunnning the training. \n",
    "\n",
    "It is obtained from internal algorithm quantities, like number of time a feature is used to define leaf, and the information gained from the nodes that use that feature.\n",
    "\n",
    "Magnitude is arbitrary. It not always a reliable indication of which feature is the most discriminant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm._Booster.dump_model()['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.bar(data.columns.values, xgb.feature_importances_)\n",
    "lightgbm.plot_importance(booster=gbm, \n",
    "                         importance_type='gain',\n",
    "                        title= \"Feature importance (gain)\",\n",
    "                        )\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.title(\"Feature importances Light GBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7WxH3bDrlIp"
   },
   "source": [
    "# Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55ZOBmxxyd0w"
   },
   "source": [
    "A better way to show the importance of each feature is Permutation Importance, where each feature in turn is replaced by an instance of that feature from another event (effectively switching it off by randomising).\n",
    "In particular it allows one to : \n",
    "   * display directly the loss in whatever criteria (ROC auc, asimov significance) when the feature is switched off\n",
    "   * display the feature importance for a specific subset (for example the most signal like)\n",
    "   * it can even display which feature has the larges impact on systematics\n",
    "\n",
    "\n",
    "However, report can be misleading in case of highly correlated variables. \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from permutationimportancephysics.PermutationImportance import PermulationImportance\n",
    "pi = PermulationImportance(model=gbm, X=X_test,y=y_test,weights=weights_test,n_iterations=3,usePredict_poba=True,\n",
    "                      scoreFunction=\"amsasimov\", colNames=data.columns)\n",
    "plt = pi.plotBars()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Plot without Renormalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_train_test(\n",
    "    y_pred_train_gbm,\n",
    "    y_train,\n",
    "    y_pred_gbm,\n",
    "    y_test,\n",
    "    xlabel=\"LGBM score\",\n",
    "    ylabel=\"Expected number of events\",\n",
    "    title=\"LGBM\",\n",
    "    weights_train=weights_train.values,\n",
    "    weights_test=weights_test.values,\n",
    "    density=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extra BDT Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "__Example with sklearn (but without weights and with no fixed test dataset):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this cell from 'raw' to 'code'\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    XGBClassifier(tree_method=\"hist\", use_label_encoder=False, eval_metric='logloss', n_estimators=10),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    train_sizes=[0.01, 0.05, 0.1, 0.2, 0.5, 0.75, 1],\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4DfF0ISrlIN"
   },
   "source": [
    "# Train BDT using [XGBoost](https://arxiv.org/abs/1603.02754)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqMCgvbkrlIN",
    "outputId": "bd68922c-8aff-47f6-a26d-8d151689e22b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(31415)  # set the random seed\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score  # for binary classification if x > 0.5 -> 1 else -> 0\n",
    "\n",
    "xgb = XGBClassifier(tree_method=\"hist\", use_label_encoder=False, eval_metric='logloss')\n",
    "# tree_method=\"hist\" is 10 times faster, however less robust against awkwards features (not a bad idea to double check without it)\n",
    "# can even try tree_method=\"gpu_hist\" if proper GPU installation\n",
    "# use_label_encoder and eval_metric to silence warning in 1.3.0\n",
    "\n",
    "# HPO (==Hyper Parameter Optimization), check on the web https://xgboost.readthedocs.io/ for other parameters\n",
    "# xgb = XGBClassifier(tree_method=\"hist\",use_label_encoder=False,max_depth=10,n_estimators=100)\n",
    "\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "xgb.fit(X_train, y_train.values, sample_weight=weights_train.values)  # note that XGB 1.3.X requires positive weight\n",
    "\n",
    "training_time = time.time() - starting_time\n",
    "print(\"Training time:\", training_time)\n",
    "\n",
    "y_pred_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "y_pred_xgb = y_pred_xgb.ravel()\n",
    "y_pred_train_xgb = xgb.predict_proba(X_train)[:, 1].ravel()\n",
    "auc_test_xgb = roc_auc_score(y_true=y_test, y_score=y_pred_xgb, sample_weight=weights_test)\n",
    "print(\"auc test:\", auc_test_xgb)\n",
    "print(\"auc train:\", roc_auc_score(y_true=y_train.values, y_score=y_pred_train_xgb, sample_weight=weights_train), \"\\n\")\n",
    "\n",
    "print(\"auc test without weights\", roc_auc_score(y_true=y_test, y_score=y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum S & B after certain thresholds\n",
    "int_pred_test_sig_xgb = [weights_test[(y_test == 1) & (y_pred_xgb > th_cut)].sum() for th_cut in np.linspace(0, 1, num=50)]\n",
    "int_pred_test_bkg_xgb = [weights_test[(y_test == 0) & (y_pred_xgb > th_cut)].sum() for th_cut in np.linspace(0, 1, num=50)]\n",
    "\n",
    "vamsasimov_xgb = [amsasimov(sumsig, sumbkg) for (sumsig, sumbkg) in zip(int_pred_test_sig_xgb, int_pred_test_bkg_xgb)]\n",
    "significance_xgb = max(vamsasimov_xgb)  # finds the maximum significance. Which threshold results in this significance?\n",
    "Z = significance_xgb\n",
    "print(\"Z:\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7HnCErpyd0p",
    "tags": []
   },
   "source": [
    "## SKLearn GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eriizDZkyd0p",
    "outputId": "b8faff41-c803-412e-d81e-4393f03de425"
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "# possible parameters, just take the default\n",
    "# original_params = {\n",
    "#    \"n_estimators\": 400,\n",
    "#    \"max_leaf_nodes\": 4,\n",
    "#    \"max_depth\": None,\n",
    "#    \"random_state\": 2,\n",
    "#    \"min_samples_split\": 5,\n",
    "# }\n",
    "\n",
    "skgb = ensemble.HistGradientBoostingClassifier()\n",
    "\n",
    "\n",
    "starting_time = time.time()\n",
    "\n",
    "skgb.fit(X_train, y_train.values, sample_weight=weights_train.values)\n",
    "\n",
    "\n",
    "training_time = time.time() - starting_time\n",
    "print(\"Training time:\", training_time)\n",
    "\n",
    "y_pred_skgb = skgb.predict_proba(X_test)[:, 1]\n",
    "y_pred_skgb = y_pred_skgb.ravel()\n",
    "y_pred_train_skgb = skgb.predict_proba(X_train)[:, 1].ravel()\n",
    "auc_test_skgb = roc_auc_score(y_true=y_test, y_score=y_pred_skgb, sample_weight=weights_test)\n",
    "print(\"auc test:\", auc_test_skgb)\n",
    "print(\"auc train:\", roc_auc_score(y_true=y_train.values, y_score=y_pred_train_skgb, sample_weight=weights_train))\n",
    "\n",
    "int_pred_test_sig_skgb = [weights_test[(y_test == 1) & (y_pred_skgb > th_cut)].sum() for th_cut in np.linspace(0, 1, num=50)]\n",
    "int_pred_test_bkg_skgb = [weights_test[(y_test == 0) & (y_pred_skgb > th_cut)].sum() for th_cut in np.linspace(0, 1, num=50)]\n",
    "\n",
    "vamsasimov_skgb = [amsasimov(sumsig, sumbkg) for (sumsig, sumbkg) in zip(int_pred_test_sig_skgb, int_pred_test_bkg_skgb)]\n",
    "significance_skgb = max(vamsasimov_skgb)\n",
    "Z = significance_skgb\n",
    "print(\"Z:\", Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pSxtcTGyd0q",
    "outputId": "c4b62278-c931-434a-c9da-a1c8545da921"
   },
   "outputs": [],
   "source": [
    "print('Best significance found are:')\n",
    "print('XGBoost : ', significance_xgb)\n",
    "print('LightGBM: ', significance_gbm)\n",
    "print('sklearn: ', significance_skgb)\n",
    "\n",
    "\n",
    "print('Best auc test found are:')\n",
    "print('XGBoost: ', roc_auc_score(y_true=y_test.values, y_score=y_pred_xgb, sample_weight=weights_test))\n",
    "print('LightGBM: ', roc_auc_score(y_true=y_test.values, y_score=y_pred_gbm, sample_weight=weights_test))\n",
    "print('sklearn: ', roc_auc_score(y_true=y_test.values, y_score=y_pred_skgb, sample_weight=weights_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-LB9cbErlIb",
    "tags": []
   },
   "source": [
    "## Some nice plots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nxVz32hKwqz",
    "tags": []
   },
   "source": [
    "### load score plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ay8QTiKPKwqz"
   },
   "outputs": [],
   "source": [
    "# Plot score for signal and background, comparing training and testing\n",
    "def compare_train_test(\n",
    "    y_pred_train,\n",
    "    y_train,\n",
    "    y_pred,\n",
    "    y_test,\n",
    "    high_low=(0, 1),\n",
    "    bins=30,\n",
    "    xlabel=\"\",\n",
    "    ylabel=\"Arbitrary units\",\n",
    "    title=\"\",\n",
    "    weights_train=np.array([]),\n",
    "    weights_test=np.array([]),\n",
    "    density=True,\n",
    "):\n",
    "    if weights_train.size != 0:\n",
    "        weights_train_signal = weights_train[y_train == 1]\n",
    "        weights_train_background = weights_train[y_train == 0]\n",
    "    else:\n",
    "        weights_train_signal = None\n",
    "        weights_train_background = None\n",
    "    plt.hist(\n",
    "        y_pred_train[y_train == 1],\n",
    "        color='r',\n",
    "        alpha=0.5,\n",
    "        range=high_low,\n",
    "        bins=bins,\n",
    "        histtype='stepfilled',\n",
    "        density=density,\n",
    "        label='S (train)',\n",
    "        weights=weights_train_signal,\n",
    "    )  # alpha is transparancy\n",
    "    plt.hist(\n",
    "        y_pred_train[y_train == 0],\n",
    "        color='b',\n",
    "        alpha=0.5,\n",
    "        range=high_low,\n",
    "        bins=bins,\n",
    "        histtype='stepfilled',\n",
    "        density=density,\n",
    "        label='B (train)',\n",
    "        weights=weights_train_background,\n",
    "    )\n",
    "\n",
    "    if weights_test.size != 0:\n",
    "        weights_test_signal = weights_test[y_test == 1]\n",
    "        weights_test_background = weights_test[y_test == 0]\n",
    "    else:\n",
    "        weights_test_signal = None\n",
    "        weights_test_background = None\n",
    "    hist, bins = np.histogram(y_pred[y_test == 1], bins=bins, range=high_low, density=density, weights=weights_test_signal)\n",
    "    scale = len(y_pred[y_test == 1]) / sum(hist)\n",
    "    err = np.sqrt(hist * scale) / scale\n",
    "\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.errorbar(center, hist, yerr=err, fmt='o', c='r', label='S (test)')\n",
    "\n",
    "    hist, bins = np.histogram(\n",
    "        y_pred[y_test == 0], bins=bins, range=high_low, density=density, weights=weights_test_background\n",
    "    )\n",
    "    scale = len(y_pred[y_test == 0]) / sum(hist)\n",
    "    err = np.sqrt(hist * scale) / scale\n",
    "\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.errorbar(center, hist, yerr=err, fmt='o', c='b', label='B (test)')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 852
    },
    "id": "kEQ342s-rlIe",
    "outputId": "42488b57-8ba8-4574-8f7e-81d7f347bb45"
   },
   "outputs": [],
   "source": [
    "compare_train_test(\n",
    "    y_pred_train_xgb,\n",
    "    y_train,\n",
    "    y_pred_xgb,\n",
    "    y_test,\n",
    "    xlabel=\"XGboost score\",\n",
    "    title=\"XGboost\",\n",
    "    weights_train=weights_train.values,\n",
    "    weights_test=weights_test.values,\n",
    ")\n",
    "plt.savefig(\"Score_BDT_XGBoost_Hist.pdf\")\n",
    "plt.show()\n",
    "compare_train_test(\n",
    "    y_pred_train_gbm,\n",
    "    y_train,\n",
    "    y_pred_gbm,\n",
    "    y_test,\n",
    "    xlabel=\"LightGBM score\",\n",
    "    title=\"LightGBM\",\n",
    "    weights_train=weights_train.values,\n",
    "    weights_test=weights_test.values,\n",
    ")\n",
    "plt.savefig(\"Score_BDT_LightGBM.pdf\")\n",
    "plt.show()\n",
    "compare_train_test(\n",
    "    y_pred_train_skgb,\n",
    "    y_train,\n",
    "    y_pred_skgb,\n",
    "    y_test,\n",
    "    xlabel=\"sklearn score\",\n",
    "    title=\"sklearn\",\n",
    "    weights_train=weights_train.values,\n",
    "    weights_test=weights_test.values,\n",
    ")\n",
    "plt.savefig(\"Score_BDT_sklearn.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPv67MTLyd0s",
    "tags": []
   },
   "source": [
    "### Plot without renormalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "visTZB8eyd0t",
    "outputId": "759a2ee5-bf7c-4d4e-b562-a048bb26608f"
   },
   "outputs": [],
   "source": [
    "compare_train_test(\n",
    "    y_pred_train_xgb,\n",
    "    y_train,\n",
    "    y_pred_xgb,\n",
    "    y_test,\n",
    "    xlabel=\"XGboost score\",\n",
    "    ylabel=\"Expected number of events\",\n",
    "    title=\"XGboost\",\n",
    "    weights_train=weights_train.values,\n",
    "    weights_test=weights_test.values,\n",
    "    density=False,\n",
    ")\n",
    "plt.savefig(\"Score_BDT_XGBoost_Hist.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxGjEcej8PIi",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "tUKIWu2orlIg",
    "outputId": "2cdbb6d0-e599-417f-d22d-b121dbd69e73"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "lw = 2\n",
    "\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_true=y_test, y_score=y_pred_xgb, sample_weight=weights_test.values)\n",
    "fpr_gbm, tpr_gbm, _ = roc_curve(y_true=y_test, y_score=y_pred_gbm, sample_weight=weights_test.values)\n",
    "fpr_skgb, tpr_skgb, _ = roc_curve(y_true=y_test, y_score=y_pred_skgb, sample_weight=weights_test.values)\n",
    "\n",
    "\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='darkgreen', lw=lw, label='XGBoost (AUC  = {})'.format(np.round(auc_test_xgb, decimals=2)))\n",
    "plt.plot(\n",
    "    fpr_skgb, tpr_skgb, color='darkblue', lw=lw, label='sklearn (AUC  = {})'.format(np.round(auc_test_skgb, decimals=2))\n",
    ")\n",
    "plt.plot(\n",
    "    fpr_gbm, tpr_gbm, color='darkorange', lw=lw, label='LightGBM (AUC  = {})'.format(np.round(auc_test_gbm, decimals=2))\n",
    ")\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "# import os\n",
    "# new_dir = \"Plots/Comparing\"\n",
    "# if not os.path.isdir(new_dir):\n",
    "#    os.mkdir(new_dir)\n",
    "plt.savefig(\"ROC_comparing.pdf\")\n",
    "plt.show()  # blue line = random classification -> maximize true positive rate while miniize false positive rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziQzdapn8Tnk",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Significance curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "CGF3k0KJrlIi",
    "outputId": "1cc4d5ea-95cb-474c-bf68-7aee79dc158f"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 1, num=50), vamsasimov_xgb, label='XGBoost (Z = {})'.format(np.round(significance_xgb, decimals=2)))\n",
    "plt.plot(\n",
    "    np.linspace(0, 1, num=50), vamsasimov_skgb, label='sklearn (Z = {})'.format(np.round(significance_skgb, decimals=2))\n",
    ")\n",
    "plt.plot(np.linspace(0, 1, num=50), vamsasimov_gbm, label='LightGBM (Z = {})'.format(np.round(significance_gbm, decimals=2)))\n",
    "\n",
    "\n",
    "plt.title(\"BDT Significance\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Significance\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Significance_comparing.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJI9aimTyd0v",
    "tags": []
   },
   "source": [
    "## XGBoost and LightGBM Feature Importance\n",
    "Feature importance allows to display the importance of each feature without rerunnning the training. It is obtained from internal algorithm quantities, like number of time a feature is used to define leaf, and the information gained from the nodes that use that feature. Magnitude is arbitrary. It not always a reliable indication of which feature is the most discriminant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "id": "QN2h9y5JrlIm",
    "outputId": "ba27ffcd-1819-416b-cb64-42e877649ec6"
   },
   "outputs": [],
   "source": [
    "plt.bar(data.columns.values, xgb.feature_importances_)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Feature importances XGBoost Hist\")\n",
    "# plt.savefig(new_dir + \"/VarImp_BDT_XGBoost_Hist.pdf\",bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.bar(data.columns.values, gbm.feature_importances_) # not gain\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Feature importances LightGBM\")\n",
    "# plt.savefig(new_dir + \"/VarImp_BDT_LightGBM.pdf\",bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiGDmMS-yd0x",
    "tags": []
   },
   "source": [
    "##  Model serialisation\n",
    "It is useful to be able to save a model in order to apply it without retraining. There are many ways to do it. One is to save the whole python object with joblib (beware this is not safe if the software evolves). Another is to used dedicated serialisation like the one proposed by xgboost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nlj3NPpkyd0y",
    "outputId": "6ab9d230-1377-400e-b52b-b4c8b4fc6201"
   },
   "outputs": [],
   "source": [
    "#WARNING : StandardScaler has not been saved\n",
    "# one can look into sklearn pipeline\n",
    "if False:\n",
    "    import joblib\n",
    "    myxgb = XGBClassifier(tree_method=\"hist\",use_label_encoder=False,eval_metric='logloss',n_estimators=5)\n",
    "    myxgb.fit(X_train, y_train, sample_weight=weights_train)\n",
    "\n",
    "    auc_test_xgb = roc_auc_score(y_true=y_test, y_score=myxgb.predict_proba(X_test)[:,1],sample_weight=weights_test)\n",
    " \n",
    "    # save model\n",
    "    myxgb.save_model(\"XGBoost.json\")\n",
    "\n",
    "\n",
    "    # save python object\n",
    "    joblib.dump(myxgb, \"myxgb.dat\")\n",
    "\n",
    "    print (\"myxgb score\",auc_test_xgb)\n",
    "\n",
    "    del myxgb # delete xgb object\n",
    "\n",
    "    # reload model\n",
    "    myxgb_reloaded_from_model =XGBClassifier()\n",
    "    myxgb_reloaded_from_model.load_model(\"XGBoost.json\")\n",
    "    print (\"myxgb reloaded from model\",\n",
    "           roc_auc_score(y_true=y_test, \n",
    "                         y_score=myxgb_reloaded_from_model.predict_proba(X_test)[:,1],sample_weight=weights_test)\n",
    "          )\n",
    "\n",
    "\n",
    "    # reload object\n",
    "    myxgb_reloaded_from_joblib=joblib.load(\"myxgb.dat\")\n",
    "    print (\"myxgb reloaded from object\",\n",
    "           roc_auc_score(y_true=y_test, \n",
    "                         y_score=myxgb_reloaded_from_joblib.predict_proba(X_test)[:,1],sample_weight=weights_test)\n",
    "          )\n",
    "    # dump json file\n",
    "    !python -m json.tool XGBoost.json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L50Q4Va9sHAu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HEPML_HandsOn_BDT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
